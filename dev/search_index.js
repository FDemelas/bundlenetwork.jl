var documenterSearchIndex = {"docs":
[{"location":"api/docstrings.html#API-Docstrings","page":"Docstrings","title":"API Docstrings","text":"","category":"section"},{"location":"api/docstrings.html#BundleNetworks.constructFunction","page":"Docstrings","title":"BundleNetworks.constructFunction","text":"constructFunction(data, rescaling_factor::Real, layers = [28 * 28, 20, 10])\n\nConstructs an InnerLoss function for MNIST classification.\n\nArguments\n\ndata: Tuple (x, y) containing input images and labels\nx: Images as 3D array (28, 28, batchsize) or 2D (784, batchsize)\ny: Integer labels (batch_size,) with values 0-9\nrescaling_factor::Real: Divisor for loss scaling (default: no specific default shown)\nlayers: Network architecture specification (default: [784, 20, 10])\n\nReturns\n\nInnerLoss instance ready for training or inference\n\nDefault Architecture\n\n[28 × 28, 20, 10] = [784, 20, 10]\n- Input layer: 784 neurons (28×28 pixels)\n- Hidden layer: 20 neurons\n- Output layer: 10 neurons (one per digit class)\n\nData Preprocessing\n\nPerforms several transformations on input data:\n\nLabel Encoding\n\nConverts integer labels to one-hot vectors:\n\n# Input: y = [3, 7, 2, ...]  (class indices 0-9)\n# Output: y = [[0,0,0,1,0,0,0,0,0,0],\n#              [0,0,0,0,0,0,0,1,0,0],\n#              [0,0,1,0,0,0,0,0,0,0], ...]\n#         (10 × batch_size matrix)\n\nImage Reshaping\n\nFlattens 2D images to vectors:\n\n# Input: x shape (28, 28, batch_size)\n# Output: x shape (784, batch_size)\n\nLoss Function\n\nUses negative logit cross-entropy for maximization:\n\nf(ŷ, y) = -Flux.logitcrossentropy(ŷ, y)\n\nThe negative sign converts minimization to maximization, which is the convention for AbstractConcaveFunction.\n\nCross-Entropy Loss\n\nLogit cross-entropy directly on logits (before softmax):\n\nCE(ŷ, y) = -Σᵢ yᵢ log(softmax(ŷ)ᵢ)\n\nMore numerically stable than applying softmax then computing entropy.\n\nDevice Handling\n\nAutomatically moves data to appropriate device (CPU/GPU):\n\ndevice(x): Moves images to current device\ndevice(y): Moves labels to current device\ncpu(layers): Keeps architecture spec on CPU (metadata)\n\nRescaling Factor Guidelines\n\nThe rescaling factor affects optimization dynamics:\n\nToo small (e.g., 1.0): Large gradient magnitudes, potential instability\nToo large (e.g., 10000.0): Small gradients, slow learning\nRecommended: 10-1000 depending on batch size and architecture\n\nTypical values:\n\nSmall networks: 10-50\nMedium networks: 50-200\nLarge networks: 100-1000\n\nSee Also\n\nsizeInputSpace: Compute required parameter vector size\nprediction: Inference without loss computation\nvalue_gradient: Compute loss and gradients\n\n\n\n\n\nconstructFunction(inst::cpuInstanceMCND, rescaling_factor::Real=1.0)\n\nConstructs a Lagrangian function from an MCND problem instance.\n\nArguments\n\ninst::cpuInstanceMCND: MCND problem instance from Instances package\nMust contain: K (commodities), N (nodes), E (arcs)\nNetwork topology, costs, capacities, demands\nrescaling_factor::Real: Scaling factor for objective (default: 1.0)\n\nReturns\n\nLagrangianFunctionMCND: Callable Lagrangian function\n\nInstance Requirements\n\nThe cpuInstanceMCND instance must contain:\n\nK::Int: Number of commodities\nN::Int: Number of nodes in network\nE::Int: Number of arcs (directed edges)\nNetwork structure: head, tail functions for arcs\ncost::Vector{Float32}: Arc costs (E,)\ncapacity::Vector{Float32}: Arc capacities (E,)\nDemand information: b function for node demands\n\nRescaling Factor Guidelines\n\nUsed to normalize Lagrangian values:\n\nSmall networks (K, N, E < 100): 1.0 - 10.0\nMedium networks (K, N, E < 1000): 10.0 - 100.0\nLarge networks (K, N, E > 1000): 100.0 - 1000.0\n\nChoose to keep L(z) values in range [0.1, 1000] for numerical stability.\n\nNetwork Topology Functions\n\nThe instance provides helper functions:\n\nhead(inst, arc): Head node of arc\ntail(inst, arc): Tail node of arc\nb(inst, node, commodity): Demand at node for commodity\nsizeE(inst): Number of arcs\nsizeLM(inst): Dual variable dimensions\n\nSee Also\n\nInstances package documentation\nLagrangianFunctionMCND: Resulting structure\nsizeInputSpace: Verify dual dimensions\nnumberSP: Verify arc count\n\n\n\n\n\nconstructFunction(inst::cpuInstanceGA, rescaling_factor::Real=1.0)\n\nConstructs a Lagrangian function from a problem instance.\n\nArguments\n\ninst::cpuInstanceGA: Problem instance from Instances package\nContains: I (items), J (commodities), p, w, c arrays\nrescaling_factor::Real: Scaling factor for objective (default: 1.0)\n\nReturns\n\nLagrangianFunctionGA: Callable Lagrangian function\n\nInstance Requirements\n\nThe cpuInstanceGA instance must contain:\n\nI::Int: Number of items (arcs in network)\nJ::Int: Number of commodities\np::Matrix{Float32}: Profit matrix (I × J)\nw::Matrix{Float32}: Weight matrix (I × J)\nc::Vector{Float32}: Capacity vector (J,)\n\nRescaling Factor\n\nUsed to normalize objective values:\n\nToo small: May cause numerical overflow\nToo large: May cause underflow or slow convergence\nRecommended: Scale to keep L(z) in range [0.1, 1000]\n\nTypical values:\n\nSmall instances (I, J < 100): 1.0 - 10.0\nMedium instances (I, J < 1000): 10.0 - 100.0\nLarge instances (I, J > 1000): 100.0 - 1000.0\n\nSee Also\n\nInstances package: Problem instance management\nLagrangianFunctionGA: Resulting structure\nsizeInputSpace: Verify dimension\n\n\n\n\n\nconstructFunction(inst::Instances.TUC, rescaling_factor::Real=1.0)\n\nConstructs a Lagrangian function from a unit commitment problem instance.\n\nArguments\n\ninst::Instances.TUC: UC problem instance from Instances package\nMust contain: I (units), T (periods), D (demand), costs, constraints\nrescaling_factor::Real: Scaling factor for objective (default: 1.0)\n\nReturns\n\nLagrangianFunctionUC: Callable Lagrangian function\n\nInstance Requirements\n\nThe Instances.TUC instance must contain:\n\nI::Int: Number of generating units\nT::Int: Number of time periods (horizon length)\nD::Vector{Float32}: Demand profile (T,)\nUnit characteristics for each generator:\nCapacity limits (min/max generation)\nStartup/shutdown costs\nGeneration cost curves\nMinimum up/down times\nRamping rates\n\nRescaling Factor Guidelines\n\nUsed to normalize Lagrangian values for numerical stability:\n\nSmall systems (I, T < 10): 1.0 - 100.0 Medium systems (I, T < 100): 100.0 - 1000.0 Large systems (I, T > 100): 1000.0 - 10000.0\n\nChoose to keep L(z) values in range [0.1, 10000] for stable gradients.\n\nSee Also\n\nInstances.TUC: Instance structure documentation\nLagrangianFunctionUC: Resulting structure\nsizeInputSpace: Number of time periods (T)\nnumberSP: Number of units (I)\nUnit commitment problem formulation\n\n\n\n\n\n","category":"function"},{"location":"api/docstrings.html#BundleNetworks.sizeInputSpace","page":"Docstrings","title":"BundleNetworks.sizeInputSpace","text":"sizeInputSpace(l::InnerLoss)\n\nComputes the dimension of the parameter space for the neural network.\n\nArguments\n\nl::InnerLoss: The inner loss function structure\n\nReturns\n\nInt: Total number of parameters in the network\n\nFormula\n\nFor a network with architecture [d₁, d₂, ..., dₙ]:\n\nTotal parameters = Σᵢ₌₁ⁿ⁻¹ (dᵢ × dᵢ₊₁ + dᵢ₊₁)\n                 = Σᵢ₌₁ⁿ⁻¹ dᵢ₊₁(dᵢ + 1)\n\nWhere:\n\ndᵢ × dᵢ₊₁: Weight matrix parameters for layer i\ndᵢ₊₁: Bias vector parameters for layer i\n\nBreakdown by Layer\n\nFor each layer connection i → i+1:\n\nWeights: layers[i] × layers[i+1] parameters\nBiases: layers[i+1] parameters\nTotal: layers[i] × layers[i+1] + layers[i+1]\n\nRelationship to Network Capacity\n\nThe parameter count determines:\n\nModel capacity: More parameters → more expressiveness\nMemory requirements: Linear in parameter count\nTraining time: More parameters → slower training\nOverfitting risk: More parameters → higher risk with small datasets\n\nSee Also\n\nconstructFunction: Creates InnerLoss with specified architecture\nParameter initialization strategies\nNetwork architecture design\n\n\n\n\n\nsizeInputSpace(ϕ::LagrangianFunctionMCND)\n\nReturns the dimensions of the dual variable space.\n\nArguments\n\nϕ::LagrangianFunctionMCND: The Lagrangian function\n\nReturns\n\n(K, N): Tuple of dimensions\nK: Number of commodities\nN: Number of nodes in network\n\nExplanation\n\nThe Lagrangian has one dual variable per (commodity, node) pair:\n\nEach commodity k has N dual variables (one per node)\nTotal: K × N dual variables\n\nThese correspond to the flow conservation constraints at each node for each commodity.\n\nRelationship to Instance\n\nsizeInputSpace(ϕ) == sizeLM(ϕ.inst) == (K, N)\n\nSee Also\n\nsizeLM: Underlying function from Instances package\nnumberSP: Number of arcs (subproblems)\nNetwork dimensions and complexity\n\n\n\n\n\nsizeInputSpace(ϕ::LagrangianFunctionGA)\n\nReturns the dimension of the dual variable space.\n\nArguments\n\nϕ::LagrangianFunctionGA: The Lagrangian function\n\nReturns\n\nInt: Dimension of dual variables (equals number of items I)\n\nExplanation\n\nThe Lagrangian has one dual variable per relaxed constraint. For the knapsack relaxation of MCND:\n\nEach item (arc) has one dual variable\nTotal: I dual variables\n\nRelationship to Instance\n\nsizeInputSpace(ϕ) == ϕ.inst.I == sizeLM(ϕ.inst)\n\nWhere:\n\nI: Number of items in the problem\nsizeLM: Size of Lagrange multiplier vector\n\nSee Also\n\nsizeLM: Underlying function from Instances package\nnumberSP: Returns number of subproblems (J)\n\n\n\n\n\nsizeInputSpace(ϕ::LagrangianFunctionUC)\n\nReturns the dimension of the dual variable space (number of time periods).\n\nArguments\n\nϕ::LagrangianFunctionUC: The Lagrangian function\n\nReturns\n\nInt: Number of time periods T in the planning horizon\n\nExplanation\n\nThe Lagrangian has one dual variable (price) per time period:\n\nEach time period t has one demand constraint\nEach constraint has one Lagrange multiplier z[t]\nTotal: T dual variables\n\nThese represent electricity prices at each time period.\n\nRelationship to Instance\n\nsizeInputSpace(ϕ) == Instances.nT(ϕ.inst) == T\n\nSee Also\n\nInstances.nT: Underlying function from Instances package\nnumberSP: Number of generators (I)\nUC problem horizon selection\nTemporal resolution trade-offs\n\n\n\n\n\n","category":"function"},{"location":"api/docstrings.html#BundleNetworks.value_gradient","page":"Docstrings","title":"BundleNetworks.value_gradient","text":"value_gradient(ϕ::AbstractConcaveFunction, z::AbstractArray)\n\nComputes both the function value and subgradient for a concave function.\n\nArguments\n\nϕ::AbstractConcaveFunction: The concave function to evaluate\nz::AbstractArray: The point at which to evaluate the function\n\nReturns\n\nvalue: The function value ϕ(z)\ngradient: A subgradient ∂ϕ(z) (or gradient if ϕ is differentiable)\n\nMathematical Background\n\nFor a concave function ϕ, a vector g is a subgradient at point z if:\n\nϕ(y) ≤ ϕ(z) + g'(y - z)  for all y\n\nFor differentiable concave functions, the gradient is the unique subgradient.\n\nImplementation Details\n\nCPU Transfer: Input is moved to CPU for computation\nAutomatic Differentiation: Uses Flux.withgradient for gradient computation\nDevice Transfer: Results are moved back to appropriate device (CPU/GPU)\n\nWhy CPU Transfer?\n\nSome operations may be more stable or only supported on CPU. The function ensures compatibility by:\n\nMoving input to CPU before computation\nComputing on CPU\nMoving results back to the original device\n\nNumerical Considerations\n\nGradients are computed using automatic differentiation\nFor non-differentiable points, returns one valid subgradient\nEnsures type consistency (Float32) for GPU compatibility\n\nSee Also\n\nChainRulesCore.rrule: Custom backward pass implementation\nAbstractConcaveFunction: Base type requiring this interface\n\n\n\n\n\nvalue_gradient(ϕ::InnerLoss, z::AbstractArray)\n\nComputes loss value and gradient for the inner loss function.\n\nArguments\n\nϕ::InnerLoss: The inner loss function\nz::AbstractArray: Network parameters (flattened vector)\n\nReturns\n\nvalue: Loss value (scalar)\ngradient: Gradient w.r.t. parameters (same shape as z)\n\nSpecialization\n\nThis is a specialized implementation of value_gradient for InnerLoss. Unlike the default implementation in AbstractConcaveFunction, this version:\n\nKeeps data on GPU/CPU (no unnecessary transfers)\nUses device(z) to ensure parameters are on correct device\n\nWhy Override?\n\nThe default value_gradient for AbstractConcaveFunction:\n\nMoves input to CPU: z = cpu(z)\nComputes on CPU\nMoves results back: device(obj), device(grad[1])\n\nFor neural networks, this is inefficient because:\n\nNetworks benefit from GPU acceleration\nCPU ↔ GPU transfers are expensive\nGradients are large (thousands of parameters)\n\nPerformance Considerations\n\nGPU acceleration: Keeps computation on GPU throughout\nMemory efficiency: No unnecessary copies\nBatch processing: Efficiently handles batched data\n\nSee Also\n\nvalue_gradient(::AbstractConcaveFunction, ...): Default implementation\nFlux.withgradient: Automatic differentiation\n\n\n\n\n\n","category":"function"},{"location":"installation.html#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation.html#Prerequisites","page":"Installation","title":"Prerequisites","text":"Julia 1.9 or higher\n(Optional) CUDA-compatible GPU for acceleration","category":"section"},{"location":"installation.html#Installing-Julia","page":"Installation","title":"Installing Julia","text":"Download Julia from julialang.org","category":"section"},{"location":"installation.html#Linux/macOS","page":"Installation","title":"Linux/macOS","text":"wget https://julialang-s3.julialang.org/bin/linux/x64/1.9/julia-1.9.4-linux-x86_64.tar.gz\ntar zxvf julia-1.9.4-linux-x86_64.tar.gz\nexport PATH=\"$PATH:$PWD/julia-1.9.4/bin\"","category":"section"},{"location":"installation.html#Windows","page":"Installation","title":"Windows","text":"Download and run the installer from the Julia website.","category":"section"},{"location":"installation.html#Installing-BundleNetworks","page":"Installation","title":"Installing BundleNetworks","text":"","category":"section"},{"location":"installation.html#Option-1:-Clone-from-GitHub","page":"Installation","title":"Option 1: Clone from GitHub","text":"git clone https://github.com/yourusername/BundleNetworks.jl.git\ncd BundleNetworks.jl","category":"section"},{"location":"installation.html#Option-2:-Julia-Package-Manager-(if-registered)","page":"Installation","title":"Option 2: Julia Package Manager (if registered)","text":"using Pkg\nPkg.add(\"BundleNetworks\")","category":"section"},{"location":"installation.html#Dependencies","page":"Installation","title":"Dependencies","text":"Install required packages:\n\nusing Pkg\n\n# Core dependencies\nPkg.add([\n    \"BundleNetworks\",\n    \"Instances\", \n    \"Flux\",\n    \"Zygote\",\n    \"LinearAlgebra\",\n    \"Statistics\",\n])\n\n# Optional: GPU support\nPkg.add(\"CUDA\")\n\n# Utilities\nPkg.add([\n    \"JSON\",\n    \"BSON\",\n    \"ArgParse\",\n    \"Random\",\n])\n\n# Logging and visualization\nPkg.add([\n    \"TensorBoardLogger\",\n    \"Logging\",\n])\n\n# Training utilities\nPkg.add([\n    \"MLUtils\",\n    \"ParameterSchedulers\",\n    \"ChainRules\",\n    \"ChainRulesCore\",\n])","category":"section"},{"location":"installation.html#Verifying-Installation","page":"Installation","title":"Verifying Installation","text":"using BundleNetworks\nusing Flux\nusing CUDA\n\n# Check CUDA availability\nCUDA.functional()  # Should return true if GPU is available\n\n# Check BundleNetworks\nprintln(\"BundleNetworks loaded successfully!\")","category":"section"},{"location":"installation.html#Setting-Up-Data","page":"Installation","title":"Setting Up Data","text":"Create data directories:\n\nmkdir -p data\nmkdir -p golds\nmkdir -p resLogs\n\nDownload or generate problem instances\nCreate gold solutions file (if using .dat format):\n\nmkdir -p golds/MCNDforTest\n# Add your gold.json file","category":"section"},{"location":"installation.html#Next-Steps","page":"Installation","title":"Next Steps","text":"See Quick Start for your first training run\nExplore Tutorials for detailed examples","category":"section"},{"location":"tutorials/episodic_training.html#Episodic-Training-Tutorial","page":"Episodic Training","title":"Episodic Training Tutorial","text":"Episodic training processes one instance at a time, updating the model after each instance.","category":"section"},{"location":"tutorials/episodic_training.html#When-to-Use-Episodic-Training","page":"Episodic Training","title":"When to Use Episodic Training","text":"Use episodic training when:\n\nInstances vary greatly in size\nMemory is limited\nYou want online learning behavior\nTesting instance-specific adaptation","category":"section"},{"location":"tutorials/episodic_training.html#Basic-Episodic-Training","page":"Episodic Training","title":"Basic Episodic Training","text":"julia runs/train_episodic.jl \\\n  --data ./data/MCNDforTest/ \\\n  --lr 0.001 \\\n  --mti 100 \\\n  --mvi 20 \\\n  --seed 42 \\\n  --maxIT 50 \\\n  --maxEP 100","category":"section"},{"location":"tutorials/episodic_training.html#Key-Differences-from-Batch-Training","page":"Episodic Training","title":"Key Differences from Batch Training","text":"Feature Batch Training Episodic Training\nUpdate frequency Per batch Per instance\nGradient stability Higher Lower\nMemory usage Higher Lower\nTraining speed Slower per epoch Faster per epoch\nValidation maxItVal iterations 5×maxIT iterations","category":"section"},{"location":"tutorials/episodic_training.html#Configuration-Options","page":"Episodic Training","title":"Configuration Options","text":"","category":"section"},{"location":"tutorials/episodic_training.html#Initialization-Strategy","page":"Episodic Training","title":"Initialization Strategy","text":"","category":"section"},{"location":"tutorials/episodic_training.html#Zero-Initialization-(Default)","page":"Episodic Training","title":"Zero Initialization (Default)","text":"--cr_init false\n\nStart dual variables at zero.","category":"section"},{"location":"tutorials/episodic_training.html#Cutting-Plane-Relaxation","page":"Episodic Training","title":"Cutting-Plane Relaxation","text":"--cr_init true\n\nWarm-start from CR solution (slower initialization, may improve convergence).","category":"section"},{"location":"tutorials/episodic_training.html#Loss-Functions","page":"Episodic Training","title":"Loss Functions","text":"","category":"section"},{"location":"tutorials/episodic_training.html#Standard-Loss-(Default)","page":"Episodic Training","title":"Standard Loss (Default)","text":"--telescopic false\n\nLoss based on final point only.","category":"section"},{"location":"tutorials/episodic_training.html#Telescopic-Loss","page":"Episodic Training","title":"Telescopic Loss","text":"--telescopic true \\\n--gamma 0.1\n\nLoss includes all visited points: L = -Σ γ^i * ϕ(x_i)","category":"section"},{"location":"tutorials/episodic_training.html#Feature-Engineering","page":"Episodic Training","title":"Feature Engineering","text":"","category":"section"},{"location":"tutorials/episodic_training.html#Instance-Features","page":"Episodic Training","title":"Instance Features","text":"--instance_features true  # Include linear relaxation features\n--instance_features false # Use only bundle state\n\nInstance features include:\n\nLinear relaxation dual variables\nConstraint structure information\nProblem-specific characteristics","category":"section"},{"location":"tutorials/episodic_training.html#Proximity-Parameter-Strategy","page":"Episodic Training","title":"Proximity Parameter Strategy","text":"","category":"section"},{"location":"tutorials/episodic_training.html#Learned-Parameter-(Default)","page":"Episodic Training","title":"Learned Parameter (Default)","text":"--single_prediction false\n\nNeural network predicts t at each iteration.","category":"section"},{"location":"tutorials/episodic_training.html#Constant-Parameter","page":"Episodic Training","title":"Constant Parameter","text":"--single_prediction true\n\nUse fixed proximity parameter throughout.","category":"section"},{"location":"tutorials/episodic_training.html#Complete-Example","page":"Episodic Training","title":"Complete Example","text":"julia runs/train_episodic.jl \\\n  --data ./data/MCNDforTest/ \\\n  --lr 0.001 \\\n  --cn 5 \\\n  --mti 200 \\\n  --mvi 40 \\\n  --seed 42 \\\n  --maxIT 50 \\\n  --maxEP 100 \\\n  --cr_init false \\\n  --exactGrad true \\\n  --telescopic true \\\n  --gamma 0.1 \\\n  --instance_features true \\\n  --single_prediction false \\\n  --sample_inside true","category":"section"},{"location":"tutorials/episodic_training.html#Validation-Behavior","page":"Episodic Training","title":"Validation Behavior","text":"Episodic training validates at three iteration counts:\n\nmaxIT: Same as training\n2×maxIT: Medium-length run\n5×maxIT: Extended run\n\nThis helps assess:\n\nShort-term performance\nConvergence stability\nLong-term behavior","category":"section"},{"location":"tutorials/episodic_training.html#Monitoring","page":"Episodic Training","title":"Monitoring","text":"","category":"section"},{"location":"tutorials/episodic_training.html#TensorBoard-Metrics","page":"Episodic Training","title":"TensorBoard Metrics","text":"tensorboard --logdir resLogs/\n\nUnique to episodic training:\n\nValidation/GAP_percentage_li: Gap at maxIT\nValidation x2/GAP_percentage: Gap at 2×maxIT\nValidation x5/GAP_percentage: Gap at 5×maxIT","category":"section"},{"location":"tutorials/episodic_training.html#Comparing-Validation-Lengths","page":"Episodic Training","title":"Comparing Validation Lengths","text":"Good convergence pattern:\n\nmaxIT gap:   5.2%\n2×maxIT gap: 3.1%\n5×maxIT gap: 2.0%\n\nPoor convergence:\n\nmaxIT gap:   5.2%\n2×maxIT gap: 5.0%\n5×maxIT gap: 4.9%","category":"section"},{"location":"tutorials/episodic_training.html#Output-Files","page":"Episodic Training","title":"Output Files","text":"Similar to batch training, plus:\n\nValidation metrics at multiple iteration counts","category":"section"},{"location":"tutorials/episodic_training.html#Batch-vs.-Episodic:-When-to-Choose","page":"Episodic Training","title":"Batch vs. Episodic: When to Choose","text":"Choose Batch Training if:\n\n✓ Instances are similar in size\n✓ You have sufficient memory\n✓ You want stable gradients\n✓ Training time is not critical\n\nChoose Episodic Training if:\n\n✓ Instances vary greatly\n✓ Memory is limited\n✓ You want faster epoch times\n✓ Testing online learning","category":"section"},{"location":"tutorials/episodic_training.html#Next-Steps","page":"Episodic Training","title":"Next Steps","text":"Compare with Batch Training\nLearn Testing & Evaluation\nSee Hyperparameter Tuning","category":"section"},{"location":"tutorials/testing.html#Testing-and-Evaluation-Tutorial","page":"Inference & Evaluation","title":"Testing & Evaluation Tutorial","text":"Learn how to evaluate trained models on test instances.","category":"section"},{"location":"tutorials/testing.html#Basic-Testing","page":"Inference & Evaluation","title":"Basic Testing","text":"julia runs/test.jl \\\n  --data ./data/MCNDforTest/ \\\n  --model ./resLogs/BatchVersion_bs_1_seed42_.../ \\\n  --dataset ./resLogs/BatchVersion_bs_1_seed42_.../\n\nArguments:\n\n--data: Path to instance folder\n--model: Path to folder containing trained model (nnbestLV.bson or nnbest.bson)\n--dataset: Path to folder containing dataset.json","category":"section"},{"location":"tutorials/testing.html#What-the-Test-Script-Does","page":"Inference & Evaluation","title":"What the Test Script Does","text":"Load Model: Reads trained neural network from BSON file\nLoad Test Split: Reads test instance list from dataset.json\nLoad Instances: Processes each test instance\nRun Bundle Method: Solves with NN guidance for 100 iterations (default)\nCompute Metrics: Tracks objectives, times, and gaps\nSave Results: Outputs to JSON file","category":"section"},{"location":"tutorials/testing.html#Test-Configuration","page":"Inference & Evaluation","title":"Test Configuration","text":"The test script uses these default settings:\n\nIterations: 100 (can be changed via maxIT parameter in code)\nProximity parameter: 0.000001\nDevice: CPU only\nExact gradients: Enabled\nInstance features: Enabled","category":"section"},{"location":"tutorials/testing.html#Understanding-Test-Results","page":"Inference & Evaluation","title":"Understanding Test Results","text":"","category":"section"},{"location":"tutorials/testing.html#Output-File-Structure","page":"Inference & Evaluation","title":"Output File Structure","text":"File: res_test2_<dataset_name>.json\n\n{\n  \"instance1.dat\": {\n    \"time\": 1.234,\n    \"objs\": [0.0, 100.5, 150.3, ..., 245.8],\n    \"times\": [0.01, 0.02, 0.03, ..., 1.23],\n    \"gaps\": [100.0, 15.2, 5.3, ..., 1.2]\n  },\n  \"instance2.dat\": { ... }\n}\n\nFields:\n\ntime: Total solving time (seconds)\nobjs: Objective value at each iteration\ntimes: Cumulative time at each iteration\ngaps: Optimality gap at each iteration (%)","category":"section"},{"location":"tutorials/testing.html#Analyzing-Results","page":"Inference & Evaluation","title":"Analyzing Results","text":"using JSON\nusing Statistics\n\n# Load results\nresults = JSON.parsefile(\"res_test2_MCNDforTest.json\")\n\n# Compute statistics across all instances\nfinal_gaps = [data[\"gaps\"][end] for (inst, data) in results]\nfinal_times = [data[\"time\"] for (inst, data) in results]\nfinal_objs = [data[\"objs\"][end] for (inst, data) in results]\n\nprintln(\"Mean final gap: $(mean(final_gaps))%\")\nprintln(\"Median final gap: $(median(final_gaps))%\")\nprintln(\"Mean solving time: $(mean(final_times))s\")\n\n# Find best/worst instances\nsorted_gaps = sort(collect(results), by=x->x[2][\"gaps\"][end])\nprintln(\"Best instance: $(sorted_gaps[1][1])\")\nprintln(\"Worst instance: $(sorted_gaps[end][1])\")","category":"section"},{"location":"tutorials/testing.html#Visualization","page":"Inference & Evaluation","title":"Visualization","text":"","category":"section"},{"location":"tutorials/testing.html#Plot-Convergence","page":"Inference & Evaluation","title":"Plot Convergence","text":"using Plots\n\n# Load results\nresults = JSON.parsefile(\"res_test2_MCNDforTest.json\")\n\n# Plot single instance\ninstance_name = \"instance1.dat\"\ndata = results[instance_name]\n\nplot(data[\"objs\"], \n     xlabel=\"Iteration\", \n     ylabel=\"Objective Value\",\n     title=\"Convergence: $instance_name\",\n     legend=false)","category":"section"},{"location":"tutorials/testing.html#Plot-Gaps-Over-Time","page":"Inference & Evaluation","title":"Plot Gaps Over Time","text":"plot(data[\"gaps\"],\n     xlabel=\"Iteration\",\n     ylabel=\"Optimality Gap (%)\",\n     title=\"Gap Convergence\",\n     yscale=:log10,\n     legend=false)","category":"section"},{"location":"tutorials/testing.html#Compare-Multiple-Instances","page":"Inference & Evaluation","title":"Compare Multiple Instances","text":"p = plot(xlabel=\"Iteration\", ylabel=\"Gap (%)\", \n         title=\"Gap Convergence\", yscale=:log10)\n\nfor (inst, data) in results\n    plot!(p, data[\"gaps\"], alpha=0.3, label=inst)\nend\n\nplot!(p)","category":"section"},{"location":"tutorials/testing.html#Comparing-Models","page":"Inference & Evaluation","title":"Comparing Models","text":"","category":"section"},{"location":"tutorials/testing.html#Test-Multiple-Models","page":"Inference & Evaluation","title":"Test Multiple Models","text":"# Test model 1\njulia test.jl --data ./data/ --model ./resLogs/model1/ --dataset ./resLogs/model1/\n\n# Test model 2\njulia test.jl --data ./data/ --model ./resLogs/model2/ --dataset ./resLogs/model2/","category":"section"},{"location":"tutorials/testing.html#Compare-Results","page":"Inference & Evaluation","title":"Compare Results","text":"using JSON, Statistics\n\n# Load both results\nres1 = JSON.parsefile(\"res_test2_model1.json\")\nres2 = JSON.parsefile(\"res_test2_model2.json\")\n\n# Compare final gaps\ngaps1 = [data[\"gaps\"][end] for (_, data) in res1]\ngaps2 = [data[\"gaps\"][end] for (_, data) in res2]\n\nprintln(\"Model 1 mean gap: $(mean(gaps1))%\")\nprintln(\"Model 2 mean gap: $(mean(gaps2))%\")\nprintln(\"Improvement: $(mean(gaps1) - mean(gaps2))%\")\n\n# Statistical test\nusing HypothesisTests\nt_test = OneSampleTTest(gaps1 .- gaps2)\nprintln(t_test)","category":"section"},{"location":"tutorials/testing.html#Advanced:-Custom-Testing","page":"Inference & Evaluation","title":"Advanced: Custom Testing","text":"","category":"section"},{"location":"tutorials/testing.html#Modify-Test-Parameters","page":"Inference & Evaluation","title":"Modify Test Parameters","text":"Edit test.jl to change:\n\n# Change number of iterations\nmaxIT = 200  # Instead of default 100\n\n# Change proximity parameter\nt = 0.00001  # Instead of default 0.000001\n\n# Disable instance features\ninstance_features = false","category":"section"},{"location":"tutorials/testing.html#Test-on-Custom-Dataset","page":"Inference & Evaluation","title":"Test on Custom Dataset","text":"# Create custom test set\ncustom_test = [\"instance1.dat\", \"instance5.dat\", \"instance10.dat\"]\n\n# Modify test.jl or create dataset.json:\ndataset = Dict(\"test\" => custom_test)","category":"section"},{"location":"tutorials/testing.html#Performance-Benchmarking","page":"Inference & Evaluation","title":"Performance Benchmarking","text":"","category":"section"},{"location":"tutorials/testing.html#Against-Baseline","page":"Inference & Evaluation","title":"Against Baseline","text":"Compare against standard bundle method:\n\n# Test with NN guidance\njulia test.jl --data ./data/ --model ./resLogs/trained_model/ ...\n\n# Test with constant t (baseline)\n# Modify test.jl to use constant t_strat instead of nn_t_strategy()","category":"section"},{"location":"tutorials/testing.html#Metrics-to-Report","page":"Inference & Evaluation","title":"Metrics to Report","text":"Final Gap: Optimality at termination\nTime to 5% Gap: Iterations needed\nTotal Time: Computational cost\nSuccess Rate: % instances below target gap","category":"section"},{"location":"tutorials/testing.html#Troubleshooting","page":"Inference & Evaluation","title":"Troubleshooting","text":"","category":"section"},{"location":"tutorials/testing.html#Model-Not-Found","page":"Inference & Evaluation","title":"Model Not Found","text":"ERROR: SystemError: opening file \"nn_bestLV.bson\": No such file\n\nSolution: Check model path and ensure nn_bestLV.bson exists.","category":"section"},{"location":"tutorials/testing.html#Gold-Solutions-Missing","page":"Inference & Evaluation","title":"Gold Solutions Missing","text":"If using .dat format, ensure:\n\n./golds/<dataset_name>/gold.json\n\nexists with format:\n\n{\n  \"instance1.dat\": optimal_value,\n  ...\n}","category":"section"},{"location":"tutorials/testing.html#Out-of-Memory","page":"Inference & Evaluation","title":"Out of Memory","text":"Reduce batch size in test (currently always 1) or test fewer instances.","category":"section"},{"location":"tutorials/testing.html#Next-Steps","page":"Inference & Evaluation","title":"Next Steps","text":"Return to Batch Training or Episodic Training\nSee Troubleshooting for common issues\nExplore API Reference for function details","category":"section"},{"location":"api/training.html#Training-API-Reference","page":"Training Functions","title":"Training API Reference","text":"","category":"section"},{"location":"api/training.html#Batch-Training","page":"Training Functions","title":"Batch Training","text":"main(args)\nFilter = t -> parentmodule(t) === BundleNetworks","category":"section"},{"location":"api/training.html#Function:-main","page":"Training Functions","title":"Function: main","text":"Main entry point for batch training.\n\nSource: runs/train_batch.jl\n\nArguments:\n\nargs::Vector{String}: Command-line arguments\n\nRequired Arguments:\n\n--lr::Float64: Learning rate\n--mti::Int64: Maximum training instances\n--mvi::Int64: Maximum validation instances  \n--seed::Int64: Random seed\n--maxItBack::Int64: Max iterations for backward pass\n--maxEP::Int64: Maximum epochs\n\nOptional Arguments:\n\n--data::String: Instance folder path (default: \"./data/MCNDforTest/\")\n--decay::Float64: Learning rate decay (default: 0.9)\n--lambda::Float32: Final point weight (default: 0.0)\n--gamma::Float32: Telescopic weight (default: 0.0)\n--cn::Int64: Gradient clipping norm (default: 5)\n--batch_size::Int64: Batch size (default: 1)\n--h_representation::Int64: Hidden size (default: 64)\n\nOutput Files:\n\nnn.bson: Final model\nnn_best.bson: Best validation model\nloss.json, obj.json, gaps.json: Training metrics\nobj_val.json, gaps_val.json: Validation metrics\n\nExample:\n\nargs = [\n    \"--data\", \"./data/MCNDforTest/\",\n    \"--lr\", \"0.001\",\n    \"--mti\", \"100\",\n    \"--mvi\", \"20\",\n    \"--seed\", \"42\",\n    \"--maxItBack\", \"50\",\n    \"--maxEP\", \"100\"\n]\nmain(args)","category":"section"},{"location":"api/training.html#Episodic-Training","page":"Training Functions","title":"Episodic Training","text":"","category":"section"},{"location":"api/training.html#Function:-ep_train_and_val","page":"Training Functions","title":"Function: ep_train_and_val","text":"Execute episodic training and validation.\n\nSource: runs/train_episodic.jl\n\nSignature:\n\nfunction ep_train_and_val(\n    folder, directory, dataset, gold, \n    idxs_train, idxs_val, opt;\n    maxEP=10, maxIT=50, kwargs...\n)\n\nArguments:\n\nfolder::String: Instance folder path\ndirectory::Vector{String}: File list\ndataset::Vector{Tuple}: (filename, objective) pairs\ngold::Dict: Optimal solutions\nidxs_train::Vector{Int}: Training indices\nidxs_val::Vector{Int}: Validation indices\nopt: Flux optimizer\n\nKeyword Arguments:\n\nmaxEP::Int=10: Maximum epochs\nmaxIT::Int=50: Bundle iterations\ncr_init::Bool=false: Use CR initialization\nexactGrad::Bool=true: Exact gradient formula\ntelescopic::Bool=false: Telescopic loss\nγ::Float64=0.1: Telescopic weight decay\ninstance_features::Bool=false: Include instance features\n\nReturns: Nothing (saves to disk)\n\nExample:\n\nopt = Flux.OptimiserChain(Flux.Optimise.Adam(0.001), ClipNorm(5))\nep_train_and_val(\n    \"./data/\", directory, dataset, gold, \n    1:100, 101:120, opt;\n    maxEP=50, maxIT=50, telescopic=true\n)","category":"section"},{"location":"api/training.html#Function:-saveJSON","page":"Training Functions","title":"Function: saveJSON","text":"Helper to save dictionary to JSON.\n\nSignature:\n\nfunction saveJSON(name::String, res::Dict)\n\nArguments:\n\nname::String: Output file path\nres::Dict: Dictionary to save\n\nExample:\n\nresults = Dict(\"loss\" => [1.0, 0.8, 0.6])\nsaveJSON(\"results.json\", results)","category":"section"},{"location":"api/training.html#Common-Helper-Functions","page":"Training Functions","title":"Common Helper Functions","text":"","category":"section"},{"location":"api/training.html#gap","page":"Training Functions","title":"gap","text":"Calculate relative percentage gap.\n\nSignature:\n\ngap(a, b) = abs(a - b) / max(a, b) * 100\n\nArguments:\n\na::Float64: First value (solution)\nb::Float64: Second value (optimal)\n\nReturns: Float64 - Percentage gap\n\nExample:\n\nsolution = 95.0\noptimal = 100.0\ng = gap(solution, optimal)  # Returns 5.0","category":"section"},{"location":"manual/architecture.html#Neural-Network-Architecture","page":"Architecture","title":"Neural Network Architecture","text":"","category":"section"},{"location":"manual/architecture.html#Overview","page":"Architecture","title":"Overview","text":"The neural network architecture is designed to predict bundle method parameters  from the current optimization state.","category":"section"},{"location":"manual/architecture.html#Model-Components","page":"Architecture","title":"Model Components","text":"","category":"section"},{"location":"manual/architecture.html#Encoder","page":"Architecture","title":"Encoder","text":"Processes instance features and bundle state:\n\nInput Features → Linear → Activation → Linear → Hidden Representation\n\nInput features include:\n\nBundle gradients (subgradients)\nFunction values\nDual variables (optional: from CR)\nInstance structure (optional: graph features)","category":"section"},{"location":"manual/architecture.html#Decoder-for-Proximity-Parameter-(t)","page":"Architecture","title":"Decoder for Proximity Parameter (t)","text":"Predicts the proximity parameter:\n\nHidden Representation → Linear → Softplus → t\n\nOutput: Scalar proximity parameter controlling trust region size.","category":"section"},{"location":"manual/architecture.html#Decoder-for-Gradient-Aggregation-(γ)","page":"Architecture","title":"Decoder for Gradient Aggregation (γ)","text":"Predicts weights for convex combination of gradients:\n\nHidden Representation → Attention → Distribution → θ\n\nComponents:\n\nQuery Network: Generates query from hidden state\nKey Network: Generates keys from bundle components\nAttention Mechanism: Computes attention scores\nDistribution: Softmax or Sparsemax normalization\n\nOutput: Probability distribution over bundle components.","category":"section"},{"location":"manual/architecture.html#Attention-Mechanism","page":"Architecture","title":"Attention Mechanism","text":"The attention mechanism computes how to aggregate bundle gradients:\n\nQ = QueryNetwork(hidden_state)\nK = KeyNetwork(bundle_gradients)\nV = bundle_gradients\n\nscores = Q · K^T\nθ = softmax(scores)\naggregated_gradient = Σ θ_i * V_i","category":"section"},{"location":"manual/architecture.html#Architecture-Variants","page":"Architecture","title":"Architecture Variants","text":"","category":"section"},{"location":"manual/architecture.html#Standard-Architecture-(h3false)","page":"Architecture","title":"Standard Architecture (h3=false)","text":"Three separate hidden representations:\n\nOne for proximity parameter\nOne for attention queries\nOne for attention keys","category":"section"},{"location":"manual/architecture.html#Compact-Architecture-(h3true)","page":"Architecture","title":"Compact Architecture (h3=true)","text":"Single shared hidden representation for all outputs.","category":"section"},{"location":"manual/architecture.html#Model-Factory","page":"Architecture","title":"Model Factory","text":"Models are created via factories:\n\nfactory = BundleNetworks.AttentionModelFactory()\nnn = BundleNetworks.create_NN(\n    factory;\n    h_representation=64,\n    h_act=softplus,\n    sampling_θ=false,\n    sampling_t=true\n)\n\nParameters:\n\nh_representation: Hidden layer size\nh_act: Activation function\nsampling_θ: Sample attention weights\nsampling_t: Sample proximity parameter","category":"section"},{"location":"manual/architecture.html#Sampling-Strategies","page":"Architecture","title":"Sampling Strategies","text":"","category":"section"},{"location":"manual/architecture.html#Deterministic-(Default-for-Testing)","page":"Architecture","title":"Deterministic (Default for Testing)","text":"nn.sample_t = false\nnn.sample_γ = false\n\nOutputs are mean predictions.","category":"section"},{"location":"manual/architecture.html#Stochastic-(Training)","page":"Architecture","title":"Stochastic (Training)","text":"nn.sample_t = true\nnn.sample_γ = true\n\nSamples from learned distributions for exploration.","category":"section"},{"location":"manual/architecture.html#Graph-Features","page":"Architecture","title":"Graph Features","text":"When use_graph=true, bipartite graph features are extracted:\n\nInstance → Bipartite Graph → Graph Convolution → Features\n\nThis captures:\n\nVariable-constraint relationships\nNetwork structure\nSparsity patterns","category":"section"},{"location":"manual/architecture.html#Activation-Functions","page":"Architecture","title":"Activation Functions","text":"Supported activations:\n\nsoftplus: Smooth, always positive (good for t)\nrelu: Sparse, fast\ntanh: Bounded, smooth\ngelu: Modern, smooth","category":"section"},{"location":"manual/architecture.html#Parameter-Count","page":"Architecture","title":"Parameter Count","text":"Typical model sizes:\n\nh_representation Parameters\n32 ~10K\n64 ~40K\n128 ~160K","category":"section"},{"location":"manual/architecture.html#Next-Steps","page":"Architecture","title":"Next Steps","text":"See Bundle Methods for algorithm details\nLearn about Loss Functions\nExplore Hyperparameter Tuning","category":"section"},{"location":"quickstart.html#Quick-Start-Guide","page":"Quick Start","title":"Quick Start Guide","text":"This guide will help you train and test your first model in under 5 minutes.","category":"section"},{"location":"quickstart.html#Step-1:-Prepare-Your-Data","page":"Quick Start","title":"Step 1: Prepare Your Data","text":"Ensure you have:\n\nProblem instances in ./data/MCNDforTest/\n(Optional) Gold solutions in ./golds/MCNDforTest/gold.json","category":"section"},{"location":"quickstart.html#Step-2:-Train-a-Model","page":"Quick Start","title":"Step 2: Train a Model","text":"","category":"section"},{"location":"quickstart.html#Minimal-Training-Command","page":"Quick Start","title":"Minimal Training Command","text":"julia runs/train_batch.jl \\\n  --data ./data/MCNDforTest/ \\\n  --lr 0.001 \\\n  --mti 50 \\\n  --mvi 10 \\\n  --seed 42 \\\n  --maxItBack 30 \\\n  --maxEP 20\n\nParameters Explained:\n\n--lr 0.001: Learning rate\n--mti 50: Use 50 training instances\n--mvi 10: Use 10 validation instances\n--seed 42: Random seed for reproducibility\n--maxItBack 30: Unroll 30 iterations for gradient computation\n--maxEP 20: Train for 20 epochs","category":"section"},{"location":"quickstart.html#What-Happens-During-Training","page":"Quick Start","title":"What Happens During Training","text":"Data Loading: Instances are loaded and rescaled\nModel Initialization: Neural network is created\nTraining Loop: For each epoch:\nShuffle training data\nProcess batches\nCompute gradients via unrolling\nUpdate model parameters\nValidate on validation set\nSaving: Best model and metrics are saved","category":"section"},{"location":"quickstart.html#Expected-Output","page":"Quick Start","title":"Expected Output","text":"Epoch 1 Training - lsp: 1234.56  gap: 15.2%\nEpoch 1 Validation - lsp: 1250.30  gap: 12.8%\nEpoch 2 Training - lsp: 1450.23  gap: 8.5%\nEpoch 2 Validation - lsp: 1480.15  gap: 7.2%\n...\n\nResults are saved to: resLogs/BatchVersion_bs_1_seed42_.../","category":"section"},{"location":"quickstart.html#Step-3:-Monitor-Training","page":"Quick Start","title":"Step 3: Monitor Training","text":"","category":"section"},{"location":"quickstart.html#Using-TensorBoard","page":"Quick Start","title":"Using TensorBoard","text":"tensorboard --logdir resLogs/\n\nOpen browser to: http://localhost:6006","category":"section"},{"location":"quickstart.html#Key-Metrics-to-Watch","page":"Quick Start","title":"Key Metrics to Watch","text":"GAP_percentage: Optimality gap (lower is better)\nLSP_value: Objective value (higher is better)\nLoss_value: Training loss","category":"section"},{"location":"quickstart.html#Step-4:-Test-Your-Model","page":"Quick Start","title":"Step 4: Test Your Model","text":"julia runs/test.jl \\\n  --data ./data/MCNDforTest/ \\\n  --model ./resLogs/BatchVersion_bs_1_seed42_.../ \\\n  --dataset ./resLogs/BatchVersion_bs_1_seed42_.../","category":"section"},{"location":"quickstart.html#Test-Results","page":"Quick Start","title":"Test Results","text":"Results saved to: res_test2_MCNDforTest.json\n\n{\n  \"instance1.dat\": {\n    \"time\": 1.234,\n    \"objs\": [0.0, 100.5, 150.3, 180.2, ...],\n    \"gaps\": [100.0, 15.2, 5.3, 2.1, ...]\n  }\n}","category":"section"},{"location":"quickstart.html#Step-5:-Analyze-Results","page":"Quick Start","title":"Step 5: Analyze Results","text":"using JSON\n\n# Load results\nresults = JSON.parsefile(\"res_test2_MCNDforTest.json\")\n\n# Compute statistics\nfor (instance, data) in results\n    final_gap = data[\"gaps\"][end]\n    println(\"$instance: Final gap = $(round(final_gap, digits=2))%\")\nend","category":"section"},{"location":"quickstart.html#Next-Steps","page":"Quick Start","title":"Next Steps","text":"","category":"section"},{"location":"quickstart.html#Improve-Performance","page":"Quick Start","title":"Improve Performance","text":"Increase Training Data: Use more instances (--mti 200)\nTrain Longer: More epochs (--maxEP 100)\nTune Learning Rate: Try different values (--lr 0.0001)\nIncrease Model Capacity: Larger hidden size (--h_representation 128)","category":"section"},{"location":"quickstart.html#Advanced-Features","page":"Quick Start","title":"Advanced Features","text":"Batch Training Tutorial: Learn batch processing\nEpisodic Training Tutorial: Instance-by-instance learning\nHyperparameter Tuning: Optimize performance","category":"section"},{"location":"quickstart.html#Troubleshooting","page":"Quick Start","title":"Troubleshooting","text":"See Troubleshooting if you encounter issues.","category":"section"},{"location":"examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples.html#Complete-Workflow-Example","page":"Examples","title":"Complete Workflow Example","text":"This example shows a complete training-to-testing workflow.","category":"section"},{"location":"examples.html#Step-1:-Prepare-Data","page":"Examples","title":"Step 1: Prepare Data","text":"mkdir -p data/my_problem\n# Add your .dat or .json files","category":"section"},{"location":"examples.html#Step-2:-Train","page":"Examples","title":"Step 2: Train","text":"julia runs/train_batch.jl \\\n  --data ./data/my_problem/ \\\n  --lr 0.001 \\\n  --mti 100 \\\n  --mvi 20 \\\n  --seed 42 \\\n  --maxItBack 50 \\\n  --maxEP 100 \\\n  --batch_size 4 \\\n  --incremental true","category":"section"},{"location":"examples.html#Step-3:-Monitor","page":"Examples","title":"Step 3: Monitor","text":"tensorboard --logdir resLogs/","category":"section"},{"location":"examples.html#Step-4:-Test","page":"Examples","title":"Step 4: Test","text":"julia runs/test.jl \\\n  --data ./data/my_problem/ \\\n  --model ./resLogs/BatchVersion_.../ \\\n  --dataset ./resLogs/BatchVersion_.../","category":"section"},{"location":"examples.html#Step-5:-Analyze","page":"Examples","title":"Step 5: Analyze","text":"using JSON, Statistics, Plots\n\n# Load results\nresults = JSON.parsefile(\"res_test2_my_problem.json\")\n\n# Plot convergence\ngaps = [data[\"gaps\"] for (_, data) in results]\nplot(gaps, alpha=0.3, xlabel=\"Iteration\", ylabel=\"Gap (%)\", \n     yscale=:log10, legend=false, title=\"Test Set Convergence\")\n\n# Compute statistics\nfinal_gaps = [g[end] for g in gaps]\nprintln(\"Mean: $(mean(final_gaps))%\")\nprintln(\"Median: $(median(final_gaps))%\")","category":"section"},{"location":"examples.html#More-Examples","page":"Examples","title":"More Examples","text":"See individual tutorial pages for detailed examples:\n\nBatch Training Examples\nEpisodic Training Examples\nTesting Examples","category":"section"},{"location":"tutorials/batch_training.html#Batch-Training-Tutorial","page":"Batch Training","title":"Batch Training Tutorial","text":"This tutorial covers batch training mode, where multiple instances are processed  together for more stable gradient estimates.","category":"section"},{"location":"tutorials/batch_training.html#When-to-Use-Batch-Training","page":"Batch Training","title":"When to Use Batch Training","text":"Use batch training when:\n\nYou have sufficient memory (GPU/CPU)\nInstances are similar in size and structure\nYou want more stable gradients\nTraining time is not critical","category":"section"},{"location":"tutorials/batch_training.html#Basic-Batch-Training","page":"Batch Training","title":"Basic Batch Training","text":"","category":"section"},{"location":"tutorials/batch_training.html#Minimal-Example","page":"Batch Training","title":"Minimal Example","text":"julia runs/train_batch.jl \\\n  --data ./data/MCNDforTest/ \\\n  --lr 0.001 \\\n  --mti 100 \\\n  --mvi 20 \\\n  --seed 42 \\\n  --maxItBack 50 \\\n  --maxEP 100 \\\n  --batch_size 4","category":"section"},{"location":"tutorials/batch_training.html#Understanding-Batch-Size","page":"Batch Training","title":"Understanding Batch Size","text":"Batch Size 1 (Default):\n\nProcesses one instance at a time\nLower memory usage\nHigher gradient variance\nFaster iterations\n\nBatch Size 4-8:\n\nProcesses multiple instances together\nMore stable gradients\nHigher memory usage\nSlower iterations but better convergence\n\nChoosing Batch Size:\n\n# Memory-constrained\n--batch_size 1\n\n# Balanced\n--batch_size 4\n\n# High-memory system\n--batch_size 8","category":"section"},{"location":"tutorials/batch_training.html#Advanced-Configuration","page":"Batch Training","title":"Advanced Configuration","text":"","category":"section"},{"location":"tutorials/batch_training.html#Curriculum-Learning","page":"Batch Training","title":"Curriculum Learning","text":"Gradually increase difficulty by starting with fewer iterations:\n\njulia runs/train_batch.jl \\\n  --incremental true \\\n  --maxIt 100 \\\n  --maxEP 100 \\\n  ...\n\nHow it works:\n\nEpochs 1-50: Linearly increase from 2 iterations to 100\nEpochs 51-100: Use full 100 iterations\n\nBenefits:\n\nEasier optimization early in training\nHelps avoid local minima\nCan improve final performance","category":"section"},{"location":"tutorials/batch_training.html#Loss-Functions","page":"Batch Training","title":"Loss Functions","text":"","category":"section"},{"location":"tutorials/batch_training.html#Standard-Loss","page":"Batch Training","title":"Standard Loss","text":"--lambda 0.0 --gamma 0.0\n\nLoss = -ϕ(x_final)","category":"section"},{"location":"tutorials/batch_training.html#Weighted-Loss","page":"Batch Training","title":"Weighted Loss","text":"--lambda 0.5\n\nLoss = -[0.5 * ϕ(xfinal) + 0.5 * ϕ(xstabilization)]","category":"section"},{"location":"tutorials/batch_training.html#Telescopic-Loss","page":"Batch Training","title":"Telescopic Loss","text":"--gamma 0.1\n\nLoss = -Σ γ^i * ϕ(x_i)\n\nRecommendation: Start with standard loss, add telescopic if underfitting.","category":"section"},{"location":"tutorials/batch_training.html#Architecture-Options","page":"Batch Training","title":"Architecture Options","text":"","category":"section"},{"location":"tutorials/batch_training.html#Hidden-Size","page":"Batch Training","title":"Hidden Size","text":"--h_representation 64   # Default, good balance\n--h_representation 32   # Faster, less capacity\n--h_representation 128  # Slower, more capacity","category":"section"},{"location":"tutorials/batch_training.html#Activation-Functions","page":"Batch Training","title":"Activation Functions","text":"--h_act softplus  # Smooth, default\n--h_act relu      # Sparse, faster\n--h_act tanh      # Bounded\n--h_act gelu      # Smooth, modern","category":"section"},{"location":"tutorials/batch_training.html#Sampling-Strategies","page":"Batch Training","title":"Sampling Strategies","text":"# Sample proximity parameter\n--sampling_t true\n\n# Sample in latent space for attention\n--sampling_gamma true","category":"section"},{"location":"tutorials/batch_training.html#Optimization-Settings","page":"Batch Training","title":"Optimization Settings","text":"","category":"section"},{"location":"tutorials/batch_training.html#Learning-Rate-Schedule","page":"Batch Training","title":"Learning Rate Schedule","text":"--lr 0.001          # Initial learning rate\n--decay 0.9         # Decay factor\n--scheduling_ss 100 # Apply decay every 100 epochs","category":"section"},{"location":"tutorials/batch_training.html#Gradient-Clipping","page":"Batch Training","title":"Gradient Clipping","text":"--cn 5   # Clip gradient norm to 5 (default)\n--cn 10  # More lenient clipping","category":"section"},{"location":"tutorials/batch_training.html#Complete-Example","page":"Batch Training","title":"Complete Example","text":"julia runs/train_batch.jl \\\n  --data ./data/MCNDforTest/ \\\n  --lr 0.001 \\\n  --decay 0.95 \\\n  --cn 5 \\\n  --mti 200 \\\n  --mvi 40 \\\n  --seed 42 \\\n  --maxItBack 50 \\\n  --maxIt 100 \\\n  --maxItVal 200 \\\n  --maxEP 150 \\\n  --batch_size 4 \\\n  --incremental true \\\n  --h_representation 64 \\\n  --h_act softplus \\\n  --use_softmax true \\\n  --sampling_t true \\\n  --sampling_gamma false \\\n  --gamma 0.05 \\\n  --lambda 0.0 \\\n  --scheduling_ss 50","category":"section"},{"location":"tutorials/batch_training.html#Monitoring-Training","page":"Batch Training","title":"Monitoring Training","text":"","category":"section"},{"location":"tutorials/batch_training.html#Console-Output","page":"Batch Training","title":"Console Output","text":"Epoch 1 Training - lsp: 1234.56  gap: 15.2%\nEpoch 1 Validation - lsp: 1250.30  gap: 12.8%","category":"section"},{"location":"tutorials/batch_training.html#TensorBoard","page":"Batch Training","title":"TensorBoard","text":"tensorboard --logdir resLogs/\n\nKey Plots:\n\nTrain/GAP_percentage: Training gap over time\nValidation/GAP_percentage: Validation gap\nTrain/Loss_value: Training loss","category":"section"},{"location":"tutorials/batch_training.html#Interpreting-Results","page":"Batch Training","title":"Interpreting Results","text":"Good Training:\n\nTraining gap decreases steadily\nValidation gap tracks training gap\nLoss decreases (becomes more negative)\n\nOverfitting:\n\nTraining gap << Validation gap\nValidation gap stops improving\n\nUnderfitting:\n\nBoth gaps remain high\nLoss plateaus early\n\nSolutions:\n\nOverfitting: Reduce model size, add regularization (gamma)\nUnderfitting: Increase model size, train longer","category":"section"},{"location":"tutorials/batch_training.html#Output-Files","page":"Batch Training","title":"Output Files","text":"After training, find in resLogs/<experiment_name>/:\n\nnn.bson: Final model\nnn_best.bson: Best validation model\nloss.json: Loss per epoch\ngaps.json: Training gaps\ngaps_val.json: Validation gaps\ndataset.json: Train/val split","category":"section"},{"location":"tutorials/batch_training.html#Next-Steps","page":"Batch Training","title":"Next Steps","text":"Try Episodic Training for comparison\nLearn about Testing your model\nExplore Hyperparameter Tuning","category":"section"},{"location":"tutorials/tesing.html#Testing-and-Evaluation-Tutorial","page":"Testing & Evaluation Tutorial","title":"Testing & Evaluation Tutorial","text":"Learn how to evaluate trained models on test instances.","category":"section"},{"location":"tutorials/tesing.html#Basic-Testing","page":"Testing & Evaluation Tutorial","title":"Basic Testing","text":"julia runs/test.jl \\\n  --data ./data/MCNDforTest/ \\\n  --model ./resLogs/BatchVersion_bs_1_seed42_.../ \\\n  --dataset ./resLogs/BatchVersion_bs_1_seed42_.../\n\nArguments:\n\n--data: Path to instance folder\n--model: Path to folder containing trained model (nnbestLV.bson or nnbest.bson)\n--dataset: Path to folder containing dataset.json","category":"section"},{"location":"tutorials/tesing.html#What-the-Test-Script-Does","page":"Testing & Evaluation Tutorial","title":"What the Test Script Does","text":"Load Model: Reads trained neural network from BSON file\nLoad Test Split: Reads test instance list from dataset.json\nLoad Instances: Processes each test instance\nRun Bundle Method: Solves with NN guidance for 100 iterations (default)\nCompute Metrics: Tracks objectives, times, and gaps\nSave Results: Outputs to JSON file","category":"section"},{"location":"tutorials/tesing.html#Test-Configuration","page":"Testing & Evaluation Tutorial","title":"Test Configuration","text":"The test script uses these default settings:\n\nIterations: 100 (can be changed via maxIT parameter in code)\nProximity parameter: 0.000001\nDevice: CPU only\nExact gradients: Enabled\nInstance features: Enabled","category":"section"},{"location":"tutorials/tesing.html#Understanding-Test-Results","page":"Testing & Evaluation Tutorial","title":"Understanding Test Results","text":"","category":"section"},{"location":"tutorials/tesing.html#Output-File-Structure","page":"Testing & Evaluation Tutorial","title":"Output File Structure","text":"File: res_test2_<dataset_name>.json\n\n{\n  \"instance1.dat\": {\n    \"time\": 1.234,\n    \"objs\": [0.0, 100.5, 150.3, ..., 245.8],\n    \"times\": [0.01, 0.02, 0.03, ..., 1.23],\n    \"gaps\": [100.0, 15.2, 5.3, ..., 1.2]\n  },\n  \"instance2.dat\": { ... }\n}\n\nFields:\n\ntime: Total solving time (seconds)\nobjs: Objective value at each iteration\ntimes: Cumulative time at each iteration\ngaps: Optimality gap at each iteration (%)","category":"section"},{"location":"tutorials/tesing.html#Analyzing-Results","page":"Testing & Evaluation Tutorial","title":"Analyzing Results","text":"using JSON\nusing Statistics\n\n# Load results\nresults = JSON.parsefile(\"res_test2_MCNDforTest.json\")\n\n# Compute statistics across all instances\nfinal_gaps = [data[\"gaps\"][end] for (inst, data) in results]\nfinal_times = [data[\"time\"] for (inst, data) in results]\nfinal_objs = [data[\"objs\"][end] for (inst, data) in results]\n\nprintln(\"Mean final gap: $(mean(final_gaps))%\")\nprintln(\"Median final gap: $(median(final_gaps))%\")\nprintln(\"Mean solving time: $(mean(final_times))s\")\n\n# Find best/worst instances\nsorted_gaps = sort(collect(results), by=x->x[2][\"gaps\"][end])\nprintln(\"Best instance: $(sorted_gaps[1][1])\")\nprintln(\"Worst instance: $(sorted_gaps[end][1])\")","category":"section"},{"location":"tutorials/tesing.html#Visualization","page":"Testing & Evaluation Tutorial","title":"Visualization","text":"","category":"section"},{"location":"tutorials/tesing.html#Plot-Convergence","page":"Testing & Evaluation Tutorial","title":"Plot Convergence","text":"using Plots\n\n# Load results\nresults = JSON.parsefile(\"res_test2_MCNDforTest.json\")\n\n# Plot single instance\ninstance_name = \"instance1.dat\"\ndata = results[instance_name]\n\nplot(data[\"objs\"], \n     xlabel=\"Iteration\", \n     ylabel=\"Objective Value\",\n     title=\"Convergence: $instance_name\",\n     legend=false)","category":"section"},{"location":"tutorials/tesing.html#Plot-Gaps-Over-Time","page":"Testing & Evaluation Tutorial","title":"Plot Gaps Over Time","text":"plot(data[\"gaps\"],\n     xlabel=\"Iteration\",\n     ylabel=\"Optimality Gap (%)\",\n     title=\"Gap Convergence\",\n     yscale=:log10,\n     legend=false)","category":"section"},{"location":"tutorials/tesing.html#Compare-Multiple-Instances","page":"Testing & Evaluation Tutorial","title":"Compare Multiple Instances","text":"p = plot(xlabel=\"Iteration\", ylabel=\"Gap (%)\", \n         title=\"Gap Convergence\", yscale=:log10)\n\nfor (inst, data) in results\n    plot!(p, data[\"gaps\"], alpha=0.3, label=inst)\nend\n\nplot!(p)","category":"section"},{"location":"tutorials/tesing.html#Comparing-Models","page":"Testing & Evaluation Tutorial","title":"Comparing Models","text":"","category":"section"},{"location":"tutorials/tesing.html#Test-Multiple-Models","page":"Testing & Evaluation Tutorial","title":"Test Multiple Models","text":"# Test model 1\njulia test.jl --data ./data/ --model ./resLogs/model1/ --dataset ./resLogs/model1/\n\n# Test model 2\njulia test.jl --data ./data/ --model ./resLogs/model2/ --dataset ./resLogs/model2/","category":"section"},{"location":"tutorials/tesing.html#Compare-Results","page":"Testing & Evaluation Tutorial","title":"Compare Results","text":"using JSON, Statistics\n\n# Load both results\nres1 = JSON.parsefile(\"res_test2_model1.json\")\nres2 = JSON.parsefile(\"res_test2_model2.json\")\n\n# Compare final gaps\ngaps1 = [data[\"gaps\"][end] for (_, data) in res1]\ngaps2 = [data[\"gaps\"][end] for (_, data) in res2]\n\nprintln(\"Model 1 mean gap: $(mean(gaps1))%\")\nprintln(\"Model 2 mean gap: $(mean(gaps2))%\")\nprintln(\"Improvement: $(mean(gaps1) - mean(gaps2))%\")\n\n# Statistical test\nusing HypothesisTests\nt_test = OneSampleTTest(gaps1 .- gaps2)\nprintln(t_test)","category":"section"},{"location":"tutorials/tesing.html#Advanced:-Custom-Testing","page":"Testing & Evaluation Tutorial","title":"Advanced: Custom Testing","text":"","category":"section"},{"location":"tutorials/tesing.html#Modify-Test-Parameters","page":"Testing & Evaluation Tutorial","title":"Modify Test Parameters","text":"Edit test.jl to change:\n\n# Change number of iterations\nmaxIT = 200  # Instead of default 100\n\n# Change proximity parameter\nt = 0.00001  # Instead of default 0.000001\n\n# Disable instance features\ninstance_features = false","category":"section"},{"location":"tutorials/tesing.html#Test-on-Custom-Dataset","page":"Testing & Evaluation Tutorial","title":"Test on Custom Dataset","text":"# Create custom test set\ncustom_test = [\"instance1.dat\", \"instance5.dat\", \"instance10.dat\"]\n\n# Modify test.jl or create dataset.json:\ndataset = Dict(\"test\" => custom_test)","category":"section"},{"location":"tutorials/tesing.html#Performance-Benchmarking","page":"Testing & Evaluation Tutorial","title":"Performance Benchmarking","text":"","category":"section"},{"location":"tutorials/tesing.html#Against-Baseline","page":"Testing & Evaluation Tutorial","title":"Against Baseline","text":"Compare against standard bundle method:\n\n# Test with NN guidance\njulia test.jl --data ./data/ --model ./resLogs/trained_model/ ...\n\n# Test with constant t (baseline)\n# Modify test.jl to use constant t_strat instead of nn_t_strategy()","category":"section"},{"location":"tutorials/tesing.html#Metrics-to-Report","page":"Testing & Evaluation Tutorial","title":"Metrics to Report","text":"Final Gap: Optimality at termination\nTime to 5% Gap: Iterations needed\nTotal Time: Computational cost\nSuccess Rate: % instances below target gap","category":"section"},{"location":"tutorials/tesing.html#Troubleshooting","page":"Testing & Evaluation Tutorial","title":"Troubleshooting","text":"","category":"section"},{"location":"tutorials/tesing.html#Model-Not-Found","page":"Testing & Evaluation Tutorial","title":"Model Not Found","text":"ERROR: SystemError: opening file \"nn_bestLV.bson\": No such file\n\nSolution: Check model path and ensure nn_bestLV.bson exists.","category":"section"},{"location":"tutorials/tesing.html#Gold-Solutions-Missing","page":"Testing & Evaluation Tutorial","title":"Gold Solutions Missing","text":"If using .dat format, ensure:\n\n./golds/<dataset_name>/gold.json\n\nexists with format:\n\n{\n  \"instance1.dat\": optimal_value,\n  ...\n}","category":"section"},{"location":"tutorials/tesing.html#Out-of-Memory","page":"Testing & Evaluation Tutorial","title":"Out of Memory","text":"Reduce batch size in test (currently always 1) or test fewer instances.","category":"section"},{"location":"tutorials/tesing.html#Next-Steps","page":"Testing & Evaluation Tutorial","title":"Next Steps","text":"Return to Batch Training or Episodic Training\nSee Troubleshooting for common issues\nExplore API Reference for function details","category":"section"},{"location":"index.html#BundleNetworks.jl-Documentation","page":"Home","title":"BundleNetworks.jl Documentation","text":"Neural Network-Guided Bundle Methods for Non-Smooth Optimization","category":"section"},{"location":"index.html#Overview","page":"Home","title":"Overview","text":"BundleNetworks.jl implements a machine learning approach to accelerate bundle methods  by learning optimal hyperparameters from training data. The neural network predicts  proximity parameters and gradient aggregation weights at each iteration, improving  convergence speed compared to traditional bundle methods.","category":"section"},{"location":"index.html#Features","page":"Home","title":"Features","text":"Neural Network-Guided Optimization: Learn bundle method parameters using attention mechanisms\nFlexible Training: Batch and episodic training modes\nCurriculum Learning: Gradually increase problem difficulty\nComprehensive Evaluation: Training, validation, and test tracking with TensorBoard integration\nGPU Support: Optional CUDA acceleration","category":"section"},{"location":"index.html#Quick-Example","page":"Home","title":"Quick Example","text":"# Train a model\njulia train_batch.jl \\\n  --data ./data/MCNDforTest/ \\\n  --lr 0.001 \\\n  --mti 100 \\\n  --mvi 20 \\\n  --seed 42 \\\n  --maxItBack 50 \\\n  --maxEP 100\n\n# Test the model\njulia test.jl \\\n  --data ./data/MCNDforTest/ \\\n  --model ./resLogs/model_folder/ \\\n  --dataset ./resLogs/model_folder/","category":"section"},{"location":"index.html#Package-Contents","page":"Home","title":"Package Contents","text":"Pages = [\n    \"installation.md\",\n    \"quickstart.md\",\n]\nDepth = 2","category":"section"},{"location":"index.html#Manual-Outline","page":"Home","title":"Manual Outline","text":"Pages = [\n    \"manual/architecture.md\",\n    \"manual/bundle_methods.md\",\n    \"manual/data_formats.md\",\n]\nDepth = 1","category":"section"},{"location":"index.html#Index","page":"Home","title":"Index","text":"","category":"section"}]
}
