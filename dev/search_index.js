var documenterSearchIndex = {"docs":
[{"location":"tutorials/baselines.html#Baselines","page":"Baselines","title":"Baselines","text":"To compare our approaches with other existing methods, we propose several baselines:\n\nThe classic bundle method with heuristic strategies to tune the regularization parameter. Further details can be found: https://lipn.univ-paris13.fr/~demelas/Manuscript_Final.pdf\nA Flux implementation of the classic gradient descent.\nA Flux implementation of Adam optimizer.\n\nFor each baseline, we consider an initial (step-size/regularization) parameter obtained through a grid search, and we save all the results in a JSON file.\n\nAssuming that you have already downloaded or created your data and saved it in ./data/<your_folder>/, you can run the baselines as:\n\njulia --project=. ./runs/runBaselines.jl --folder ./data/<your_folder>/ --maxIterDescentType 1000 --maxIterBundle 100 --TS 0.01 0.1 1.0 1 10 100 1000","category":"section"},{"location":"tutorials/baselines.html#Parameters-Explanation","page":"Baselines","title":"Parameters Explanation","text":"--folder ./data/<your_folder>/: The folder containing your data.\n--maxIterDescentType 1000: The maximum number of iterations for Adam and Descent.\n--maxIterBundle 100: The maximum number of iterations for the Bundle methods.\n--TS 0.01 0.1 1.0 1 10 100 1000: The initial parameters to consider in the grid search.","category":"section"},{"location":"tutorials/baselines.html#Further-References","page":"Baselines","title":"Further References","text":"Bundle Network: A fully ML-based Bundle Method.\nHyper-Parameter Learning: Learning an hyper-parameter in the (aggregated) Bundle method.","category":"section"},{"location":"manual/architecture.html#WIP","page":"Architecture","title":"WIP","text":"[...]","category":"section"},{"location":"manual/architecture.html#Further-references","page":"Architecture","title":"Further references","text":"Bundle Network: A fully ML-based Bundle Method\nHyper Parameter Learning: Learning an Hyper-parameter in the (aggregated) Bundle method\nBaselines: Grid-Search based baselines including: classic (aggregated) Bundle method with different t-strategies (tuning strategies for the regularization parameter), Adam and Descent.","category":"section"},{"location":"quickstart.html#Quick-Start-Guide","page":"Quick Start","title":"Quick Start Guide","text":"This guide will help you train and test your first model in under 5 minutes.","category":"section"},{"location":"quickstart.html#Step-1:-Prepare-Your-Data","page":"Quick Start","title":"Step 1: Prepare Your Data","text":"Ensure you have:\n\nProblem instances in ./data/<your_folder>/\n(Optional) Gold solutions as json as ./golds/<your_folder>/gold.json","category":"section"},{"location":"quickstart.html#Step-2:-Train-a-Model","page":"Quick Start","title":"Step 2: Train a Model","text":"We suggest to visit the other pages to have more informations about training routines:","category":"section"},{"location":"quickstart.html#Further-references","page":"Quick Start","title":"Further references","text":"Bundle Network: A fully ML-based Bundle Method\nHyper-Parameter Learning: Learning an Hyper-parameter in the (aggregated) Bundle method\nBaselines: Grid-Search based baselines including: classic (aggregated) Bundle method with different t-strategies (tuning strategies for the regularization parameter), Adam and Descent.","category":"section"},{"location":"installation.html#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation.html#Prerequisites","page":"Installation","title":"Prerequisites","text":"Julia 1.9 or higher\n(Optional) CUDA-compatible GPU for acceleration","category":"section"},{"location":"installation.html#Installing-Julia","page":"Installation","title":"Installing Julia","text":"Download Julia from julialang.org","category":"section"},{"location":"installation.html#Linux/macOS","page":"Installation","title":"Linux/macOS","text":"wget https://julialang-s3.julialang.org/bin/linux/x64/1.9/julia-1.9.4-linux-x86_64.tar.gz\ntar zxvf julia-1.9.4-linux-x86_64.tar.gz\nexport PATH=\"$PATH:$PWD/julia-1.9.4/bin\"","category":"section"},{"location":"installation.html#Windows","page":"Installation","title":"Windows","text":"Download and run the installer from the Julia website.","category":"section"},{"location":"installation.html#Installing-BundleNetworks","page":"Installation","title":"Installing BundleNetworks","text":"","category":"section"},{"location":"installation.html#Step-1:-Clone-from-GitHub","page":"Installation","title":"Step 1: Clone from GitHub","text":"git clone git@github.com:FDemelas/bundlenetwork.jl.git\ncd bundlenetwork.jl","category":"section"},{"location":"installation.html#Step-2:-Julia-Package-Manager-(if-registered)","page":"Installation","title":"Step 2: Julia Package Manager (if registered)","text":"# Use the package manager\nusing Pkg\n\n# Activate the project directory\nPkg.activate(\".\")\n\n# Install dependencies\nPkg.instantiate()","category":"section"},{"location":"installation.html#Verifying-Installation","page":"Installation","title":"Verifying Installation","text":"using BundleNetworks\nusing Flux\nusing CUDA\n\n# Check CUDA availability\nCUDA.functional()  # Should return true if GPU is available\n\n# Confirm BundleNetworks loads successfully\nprintln(\"BundleNetworks loaded successfully!\")","category":"section"},{"location":"installation.html#Setting-Up-Data","page":"Installation","title":"Setting Up Data","text":"Create data directories:\n\nmkdir -p data\nmkdir -p golds\nmkdir -p resLogs\n\nDownload or generate problem instances and place them in ./data/<your_folder>\nCreate gold solutions file in ./golds/<your_folder>. If the directory does not exist, create it:\n\nmkdir -p golds/<your_folder>\n# Add your gold.json file here","category":"section"},{"location":"installation.html#Next-Steps","page":"Installation","title":"Next Steps","text":"See Quick Start for your first training run\nExplore Tutorials for detailed examples","category":"section"},{"location":"tutorials/bundle_networks.html#Bundle-Network","page":"Bundle Network","title":"Bundle Network","text":"This methodology substitutes the resolution of the (Quadratic) Master Problem in the Bundle Method, computing the search direction as a convex combination of the sub-gradients at visited points, with an ML model based on Recursion and Attention.\n\nWe assume that you have already downloaded or created your data and saved it in ./data/<your_folder>/.   This folder should contain 2000 instances, with 1000 used for training, 500 for validation, and 500 for testing.\n\n","category":"section"},{"location":"tutorials/bundle_networks.html#Training","page":"Bundle Network","title":"Training","text":"Here we demonstrate how to train such a model, with or without batching.","category":"section"},{"location":"tutorials/bundle_networks.html#Without-Batching","page":"Bundle Network","title":"Without Batching","text":"julia --project=. ./runs/runTraining.jl --data ./data/<your_folder>/ --lr 1.0e-4 --decay 0.9 --cn 5 --mti 1000 --mvi 500 --seed 1 --maxIt 10 --maxEP 10 --soft_updates true --h_representation 32 --use_softmax false --gamma 0.999 --lambda 0.0 --delta 0.0 --use_graph false --maxItBack -1 --maxItVal 20 --batch_size 2 --always_batch true --h_act softplus --sampling_gamma false","category":"section"},{"location":"tutorials/bundle_networks.html#Using-Batching","page":"Bundle Network","title":"Using Batching","text":"julia --project=. ./runs/runTraining.jl --data ./data/<your_folder>/ --lr 1.0e-4 --decay 0.9 --cn 5 --mti 100 --mvi 500 --seed 1 --maxIt 10 --maxEP 10 --soft_updates true --h_representation 32 --use_softmax false --gamma 0.999 --lambda 0.0 --delta 0.0 --use_graph false --maxItBack -1 --maxItVal 20 --batch_size 1 --always_batch true --h_act softplus --sampling_gamma false\n\n","category":"section"},{"location":"tutorials/bundle_networks.html#Parameter-Explanation","page":"Bundle Network","title":"Parameter Explanation","text":"--data ./data/<your_folder>/: Path to the folder containing the dataset instances.  \n--lr 1.0e-4: Learning rate for updating the model parameters.  \n--decay 0.9: Decay factor for the optimizer.  \n--cn 5: Coefficient to prevent the gradient norm from becoming too large.  \n--mti 1000: Number of training instances.  \n--mvi 500: Number of validation instances.  \n--seed 1: Random seed.  \n--maxIt 10: Number of iterations (unrolls) of the Bundle method per call.  \n--maxEP 10: Number of training epochs.  \n--soft_updates true: If false, uses the standard bundle strategy; if true, uses a smoothed update based on the softplus function.  \n--h_representation 32: Size of the hidden representation for each visited point.  \n--use_softmax false: If true, uses softmax to predict the convex combination coefficients; otherwise, uses sparsemax.  \n--gamma 0.999: Coefficient in the telescopic sum, larger for the last point, smaller for earlier points.  \n--lambda 0.0: Final contribution weight: lambda * final_point + (1 - lambda) * stabilization_point.  \n--delta 0.0: Additional regularization parameter.  \n--use_graph false: If true, uses a bipartite graph representation instead of recurrence (still in development; not recommended).  \n--maxItBack -1: (Description missing; please clarify if needed.)  \n--maxItVal 20: Number of iterations of the Bundle method for validation instances.  \n--batch_size 1: Batch size.  \n--sampling_gamma false: If true, uses sampling in the hidden representation; otherwise, no.  \n--always_batch true: If batch_size is 1 and this is true, batching is enforced anyway.  \n--h_act softplus: Activation function used in hidden layers.\n\n","category":"section"},{"location":"tutorials/bundle_networks.html#Testing","page":"Bundle Network","title":"Testing","text":"To evaluate a pretrained model's performance:\n\njulia --project=. ./runs/runTest.jl --folder ./data/<your_folder>/ --model_folder BatchVersion_bs_1_true_<your_folder>_0.0001_0.9_5_1000_500_1_10_100_true_32_false_softplus_false_0.999_0.0_0.0_sparsemax_false_false_1.0\n\n","category":"section"},{"location":"tutorials/bundle_networks.html#Parameters-Explanation","page":"Bundle Network","title":"Parameters Explanation","text":"--data ./data/<your_folder>/: Path to the dataset folder.  \n--dataset ./res/BatchVersion_bs_1_true_<your_folder>_0.0001_0.9_5_1000_500_1_10_100_true_32_false_softplus_false_0.999_0.0_0.0_sparsemax_false_false_1.0/: Path to the model folder, usually obtained from training.  \n--model: (Optional) Path to a specific model folder; if omitted, defaults to the associated test instances folder.\n\n","category":"section"},{"location":"tutorials/bundle_networks.html#Further-References","page":"Bundle Network","title":"Further References","text":"Hyper-Parameter Learning: Learning hyperparameters in the (aggregated) Bundle method.  \nBaselines: Grid-search based baselines, including classic (aggregated) Bundle methods with various strategies, Adam, and Gradient Descent.","category":"section"},{"location":"index.html#BundleNetworks.jl-Documentation","page":"Home","title":"BundleNetworks.jl Documentation","text":"Neural Network-Guided Bundle Methods for Non-Smooth Optimization","category":"section"},{"location":"index.html#Overview","page":"Home","title":"Overview","text":"BundleNetworks.jl implements:\n\nA machine learning approach to accelerate bundle methods by learning optimal hyperparameters from training data.\nA machine learning-based unrolling model that predicts the coefficients of the convex combination of gradients (considered as step size), as well as the step size itself.","category":"section"},{"location":"index.html#Features","page":"Home","title":"Features","text":"Neural Network-Guided Optimization: Learn bundle method parameters using attention mechanisms.\nFlexible Training: Supports batch and episodic training modes.\nCurriculum Learning: Gradually increases problem difficulty.\nComprehensive Evaluation: Tracks training, validation, and testing metrics with TensorBoard integration.\nGPU Support: Optional CUDA acceleration.","category":"section"},{"location":"index.html#Quick-Example","page":"Home","title":"Quick Example","text":"# Train a model\njulia runTraining.jl \\\n  --data ./data/<your_folder>/ \\\n  --lr 0.001 \\\n  --mti 100 \\\n  --mvi 20 \\\n  --seed 42 \\\n  --maxItBack 50 \\\n  --maxEP 100\n\n# Test the model\njulia runTest.jl \\\n  --data ./data/<your_folder>/ \\\n  --model ./resLogs/model_folder/ \\\n  --dataset ./resLogs/model_folder/","category":"section"},{"location":"index.html#Package-Contents","page":"Home","title":"Package Contents","text":"Pages = [\n    \"installation.md\",\n    \"quickstart.md\",\n]\nDepth = 2","category":"section"},{"location":"index.html#Manual-Outline","page":"Home","title":"Manual Outline","text":"Pages = [\n    \"manual/architecture.md\",\n    \"manual/bundle_methods.md\",\n    \"manual/data_formats.md\",\n]\nDepth = 1","category":"section"},{"location":"index.html#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"tutorials/hyper_parameters.html#Hyper-Parameter-Learning","page":"Hyper-Parameter Learning","title":"Hyper Parameter Learning","text":"This methodology maintains the structure of the Bundle Method, particularly the resolution of the (Quadratic) Master Problem (MP).   It uses an ML model based on Recursion to compute, at each step, a regularization parameter used as a weight for the Euclidean distance with respect to the best point found so far in the MP.\n\nWe assume that you have already downloaded or created your data and saved it in ./data/<your_folder>/.   This folder should contain 2000 instances, with 1000 for training, 500 for validation, and 500 for testing.","category":"section"},{"location":"tutorials/hyper_parameters.html#Training","page":"Hyper-Parameter Learning","title":"Training","text":"Here we show how to train a model.","category":"section"},{"location":"tutorials/hyper_parameters.html#Without-Batch","page":"Hyper-Parameter Learning","title":"Without Batch","text":"julia --project=. ./runs/runTrainingT.jl --data ./data/<your_folder>/ --lr 1.0e-4 --cn 5 --mti 4 --mvi 4 --seed 1 --maxIT 10 --maxEP 10 --cr_init false --telescopic true --instance_features true --gamma 0.9 --single_prediction false","category":"section"},{"location":"tutorials/hyper_parameters.html#With-Batch","page":"Hyper-Parameter Learning","title":"With Batch","text":"No mechanism to handle batch training is implemented for this model.","category":"section"},{"location":"tutorials/hyper_parameters.html#Parameters-Explanation","page":"Hyper-Parameter Learning","title":"Parameters Explanation","text":"--data ./data/<your_folder>/ : The data folder containing the dataset of instances.\n--lr 1.0e-4 : The learning rate for updating the model parameters.\n--cn 5 : Coefficient to prevent the gradient norm from becoming too large.\n--mti 1000 : Number of training instances.\n--mvi 500 : Number of validation instances.\n--seed 1 : Random seed.\n--maxIT 10 : Number of iterations (unrolls) of the Bundle method at each call.\n--maxEP 10 : Number of training epochs.\n--cr_init false : If false, start from the zero vector; otherwise, from the dual solution of the continuous relaxation (CR). Works only in the Lagrangian relaxation (LR) setting, assuming CR is easy to solve but provides a poor bound compared to LR.\n--telescopic true : If true, considers a telescopic sum of all visited points during execution.\n--instance_features true : If true, adds features depending on the instance (static during execution).\n--gamma 0.9 : Coefficient in the telescopic sum, larger for the last point, smaller for earlier points. Null Steps have lower contribution.\n--single_prediction false : If true, uses only one prediction; otherwise, multiple ones obtained with a recurrent model.","category":"section"},{"location":"tutorials/hyper_parameters.html#Testing","page":"Hyper-Parameter Learning","title":"Testing","text":"julia --project=. ./runs/runTestT.jl --data ./data/<your_folder>/ --model ./res/res_goldLossWeights_withInstFeat_initZero_lr0.0001_cn5_maxIT10_maxEP10_data<your_folder>_exactGradtrue_gamma0.9_seed1_single_predictionfalse_0.0_ --dataset ./res/BatchVersion_bs_1_true_<your_folder>_0.0001_0.9_5_4_4_1_10_10_true_32_false_softplus_false_0.999_0.0_0.0_sparsemax_false_false_1.0/","category":"section"},{"location":"tutorials/hyper_parameters.html#Parameters-Explanation-2","page":"Hyper-Parameter Learning","title":"Parameters Explanation","text":"--data ./data/<your_folder>/ : The folder containing the dataset.\n--dataset ./res/BatchVersion_bs_1_true_<your_folder>_0.0001_0.9_5_1000_500_1_10_100_true_32_false_softplus_false_0.999_0.0_0.0_sparsemax_false_false_1.0/ : The model folder, typically obtained from training.\n--model : (Optional) Path to a specific model. If not provided, defaults to the test instances associated with the training. To test a specific set, pass the path to a folder containing a dataset.json file, saved during training.","category":"section"},{"location":"tutorials/hyper_parameters.html#Further-References","page":"Hyper-Parameter Learning","title":"Further References","text":"Bundle Network: A fully ML-based Bundle Method.\nBaselines: Grid-search based baselines, including classic (aggregated) Bundle method with different T-strategies (tuning strategies for the regularization parameter), Adam, and Descent.","category":"section"}]
}
