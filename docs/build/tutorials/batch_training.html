<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Batch Training · BundleNetworks Documentation</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../index.html">BundleNetworks Documentation</a></span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Home</a></li><li><a class="tocitem" href="../installation.html">Installation</a></li><li><a class="tocitem" href="../quickstart.html">Quick Start</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href="batch_training.html">Batch Training</a><ul class="internal"><li><a class="tocitem" href="#When-to-Use-Batch-Training"><span>When to Use Batch Training</span></a></li><li><a class="tocitem" href="#Basic-Batch-Training"><span>Basic Batch Training</span></a></li><li><a class="tocitem" href="#Understanding-Batch-Size"><span>Understanding Batch Size</span></a></li><li><a class="tocitem" href="#Advanced-Configuration"><span>Advanced Configuration</span></a></li><li><a class="tocitem" href="#Complete-Example"><span>Complete Example</span></a></li><li><a class="tocitem" href="#Monitoring-Training"><span>Monitoring Training</span></a></li><li><a class="tocitem" href="#Output-Files"><span>Output Files</span></a></li><li><a class="tocitem" href="#Next-Steps"><span>Next Steps</span></a></li></ul></li><li><a class="tocitem" href="episodic_training.html">Episodic Training</a></li><li><a class="tocitem" href="testing.html">Inference &amp; Evaluation</a></li></ul></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../manual/architecture.html">Architecture</a></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../api/training.html">Training Functions</a></li></ul></li><li><a class="tocitem" href="../examples.html">Examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href="batch_training.html">Batch Training</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="batch_training.html">Batch Training</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FDemelas/bundlenetwork.jl/blob/main/docs/src/tutorials/batch_training.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Batch-Training-Tutorial"><a class="docs-heading-anchor" href="#Batch-Training-Tutorial">Batch Training Tutorial</a><a id="Batch-Training-Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#Batch-Training-Tutorial" title="Permalink"></a></h1><p>This tutorial covers batch training mode, where multiple instances are processed  together for more stable gradient estimates.</p><h2 id="When-to-Use-Batch-Training"><a class="docs-heading-anchor" href="#When-to-Use-Batch-Training">When to Use Batch Training</a><a id="When-to-Use-Batch-Training-1"></a><a class="docs-heading-anchor-permalink" href="#When-to-Use-Batch-Training" title="Permalink"></a></h2><p>Use batch training when:</p><ul><li>You have sufficient memory (GPU/CPU)</li><li>Instances are similar in size and structure</li><li>You want more stable gradients</li><li>Training time is not critical</li></ul><h2 id="Basic-Batch-Training"><a class="docs-heading-anchor" href="#Basic-Batch-Training">Basic Batch Training</a><a id="Basic-Batch-Training-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Batch-Training" title="Permalink"></a></h2><h3 id="Minimal-Example"><a class="docs-heading-anchor" href="#Minimal-Example">Minimal Example</a><a id="Minimal-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Minimal-Example" title="Permalink"></a></h3><pre><code class="language-bash hljs">julia runs/train_batch.jl \
  --data ./data/MCNDforTest/ \
  --lr 0.001 \
  --mti 100 \
  --mvi 20 \
  --seed 42 \
  --maxItBack 50 \
  --maxEP 100 \
  --batch_size 4</code></pre><h2 id="Understanding-Batch-Size"><a class="docs-heading-anchor" href="#Understanding-Batch-Size">Understanding Batch Size</a><a id="Understanding-Batch-Size-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-Batch-Size" title="Permalink"></a></h2><p><strong>Batch Size 1</strong> (Default):</p><ul><li>Processes one instance at a time</li><li>Lower memory usage</li><li>Higher gradient variance</li><li>Faster iterations</li></ul><p><strong>Batch Size 4-8</strong>:</p><ul><li>Processes multiple instances together</li><li>More stable gradients</li><li>Higher memory usage</li><li>Slower iterations but better convergence</li></ul><p><strong>Choosing Batch Size</strong>:</p><pre><code class="language-julia hljs"># Memory-constrained
--batch_size 1

# Balanced
--batch_size 4

# High-memory system
--batch_size 8</code></pre><h2 id="Advanced-Configuration"><a class="docs-heading-anchor" href="#Advanced-Configuration">Advanced Configuration</a><a id="Advanced-Configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Configuration" title="Permalink"></a></h2><h3 id="Curriculum-Learning"><a class="docs-heading-anchor" href="#Curriculum-Learning">Curriculum Learning</a><a id="Curriculum-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Curriculum-Learning" title="Permalink"></a></h3><p>Gradually increase difficulty by starting with fewer iterations:</p><pre><code class="language-bash hljs">julia runs/train_batch.jl \
  --incremental true \
  --maxIt 100 \
  --maxEP 100 \
  ...</code></pre><p><strong>How it works</strong>:</p><ul><li>Epochs 1-50: Linearly increase from 2 iterations to 100</li><li>Epochs 51-100: Use full 100 iterations</li></ul><p><strong>Benefits</strong>:</p><ul><li>Easier optimization early in training</li><li>Helps avoid local minima</li><li>Can improve final performance</li></ul><h3 id="Loss-Functions"><a class="docs-heading-anchor" href="#Loss-Functions">Loss Functions</a><a id="Loss-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-Functions" title="Permalink"></a></h3><h4 id="Standard-Loss"><a class="docs-heading-anchor" href="#Standard-Loss">Standard Loss</a><a id="Standard-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-Loss" title="Permalink"></a></h4><pre><code class="language-bash hljs">--lambda 0.0 --gamma 0.0</code></pre><p>Loss = -ϕ(x_final)</p><h4 id="Weighted-Loss"><a class="docs-heading-anchor" href="#Weighted-Loss">Weighted Loss</a><a id="Weighted-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Weighted-Loss" title="Permalink"></a></h4><pre><code class="language-bash hljs">--lambda 0.5</code></pre><p>Loss = -[0.5 * ϕ(x<em>final) + 0.5 * ϕ(x</em>stabilization)]</p><h4 id="Telescopic-Loss"><a class="docs-heading-anchor" href="#Telescopic-Loss">Telescopic Loss</a><a id="Telescopic-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Telescopic-Loss" title="Permalink"></a></h4><pre><code class="language-bash hljs">--gamma 0.1</code></pre><p>Loss = -Σ γ^i * ϕ(x_i)</p><p><strong>Recommendation</strong>: Start with standard loss, add telescopic if underfitting.</p><h3 id="Architecture-Options"><a class="docs-heading-anchor" href="#Architecture-Options">Architecture Options</a><a id="Architecture-Options-1"></a><a class="docs-heading-anchor-permalink" href="#Architecture-Options" title="Permalink"></a></h3><h4 id="Hidden-Size"><a class="docs-heading-anchor" href="#Hidden-Size">Hidden Size</a><a id="Hidden-Size-1"></a><a class="docs-heading-anchor-permalink" href="#Hidden-Size" title="Permalink"></a></h4><pre><code class="language-bash hljs">--h_representation 64   # Default, good balance
--h_representation 32   # Faster, less capacity
--h_representation 128  # Slower, more capacity</code></pre><h4 id="Activation-Functions"><a class="docs-heading-anchor" href="#Activation-Functions">Activation Functions</a><a id="Activation-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Activation-Functions" title="Permalink"></a></h4><pre><code class="language-bash hljs">--h_act softplus  # Smooth, default
--h_act relu      # Sparse, faster
--h_act tanh      # Bounded
--h_act gelu      # Smooth, modern</code></pre><h4 id="Sampling-Strategies"><a class="docs-heading-anchor" href="#Sampling-Strategies">Sampling Strategies</a><a id="Sampling-Strategies-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-Strategies" title="Permalink"></a></h4><pre><code class="language-bash hljs"># Sample proximity parameter
--sampling_t true

# Sample in latent space for attention
--sampling_gamma true</code></pre><h3 id="Optimization-Settings"><a class="docs-heading-anchor" href="#Optimization-Settings">Optimization Settings</a><a id="Optimization-Settings-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-Settings" title="Permalink"></a></h3><h4 id="Learning-Rate-Schedule"><a class="docs-heading-anchor" href="#Learning-Rate-Schedule">Learning Rate Schedule</a><a id="Learning-Rate-Schedule-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-Rate-Schedule" title="Permalink"></a></h4><pre><code class="language-bash hljs">--lr 0.001          # Initial learning rate
--decay 0.9         # Decay factor
--scheduling_ss 100 # Apply decay every 100 epochs</code></pre><h4 id="Gradient-Clipping"><a class="docs-heading-anchor" href="#Gradient-Clipping">Gradient Clipping</a><a id="Gradient-Clipping-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Clipping" title="Permalink"></a></h4><pre><code class="language-bash hljs">--cn 5   # Clip gradient norm to 5 (default)
--cn 10  # More lenient clipping</code></pre><h2 id="Complete-Example"><a class="docs-heading-anchor" href="#Complete-Example">Complete Example</a><a id="Complete-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Complete-Example" title="Permalink"></a></h2><pre><code class="language-bash hljs">julia runs/train_batch.jl \
  --data ./data/MCNDforTest/ \
  --lr 0.001 \
  --decay 0.95 \
  --cn 5 \
  --mti 200 \
  --mvi 40 \
  --seed 42 \
  --maxItBack 50 \
  --maxIt 100 \
  --maxItVal 200 \
  --maxEP 150 \
  --batch_size 4 \
  --incremental true \
  --h_representation 64 \
  --h_act softplus \
  --use_softmax true \
  --sampling_t true \
  --sampling_gamma false \
  --gamma 0.05 \
  --lambda 0.0 \
  --scheduling_ss 50</code></pre><h2 id="Monitoring-Training"><a class="docs-heading-anchor" href="#Monitoring-Training">Monitoring Training</a><a id="Monitoring-Training-1"></a><a class="docs-heading-anchor-permalink" href="#Monitoring-Training" title="Permalink"></a></h2><h3 id="Console-Output"><a class="docs-heading-anchor" href="#Console-Output">Console Output</a><a id="Console-Output-1"></a><a class="docs-heading-anchor-permalink" href="#Console-Output" title="Permalink"></a></h3><pre><code class="nohighlight hljs">Epoch 1 Training - lsp: 1234.56  gap: 15.2%
Epoch 1 Validation - lsp: 1250.30  gap: 12.8%</code></pre><h3 id="TensorBoard"><a class="docs-heading-anchor" href="#TensorBoard">TensorBoard</a><a id="TensorBoard-1"></a><a class="docs-heading-anchor-permalink" href="#TensorBoard" title="Permalink"></a></h3><pre><code class="language-bash hljs">tensorboard --logdir resLogs/</code></pre><p><strong>Key Plots</strong>:</p><ul><li><code>Train/GAP_percentage</code>: Training gap over time</li><li><code>Validation/GAP_percentage</code>: Validation gap</li><li><code>Train/Loss_value</code>: Training loss</li></ul><h3 id="Interpreting-Results"><a class="docs-heading-anchor" href="#Interpreting-Results">Interpreting Results</a><a id="Interpreting-Results-1"></a><a class="docs-heading-anchor-permalink" href="#Interpreting-Results" title="Permalink"></a></h3><p><strong>Good Training</strong>:</p><ul><li>Training gap decreases steadily</li><li>Validation gap tracks training gap</li><li>Loss decreases (becomes more negative)</li></ul><p><strong>Overfitting</strong>:</p><ul><li>Training gap &lt;&lt; Validation gap</li><li>Validation gap stops improving</li></ul><p><strong>Underfitting</strong>:</p><ul><li>Both gaps remain high</li><li>Loss plateaus early</li></ul><p><strong>Solutions</strong>:</p><ul><li>Overfitting: Reduce model size, add regularization (gamma)</li><li>Underfitting: Increase model size, train longer</li></ul><h2 id="Output-Files"><a class="docs-heading-anchor" href="#Output-Files">Output Files</a><a id="Output-Files-1"></a><a class="docs-heading-anchor-permalink" href="#Output-Files" title="Permalink"></a></h2><p>After training, find in <code>resLogs/&lt;experiment_name&gt;/</code>:</p><ul><li><code>nn.bson</code>: Final model</li><li><code>nn_best.bson</code>: Best validation model</li><li><code>loss.json</code>: Loss per epoch</li><li><code>gaps.json</code>: Training gaps</li><li><code>gaps_val.json</code>: Validation gaps</li><li><code>dataset.json</code>: Train/val split</li></ul><h2 id="Next-Steps"><a class="docs-heading-anchor" href="#Next-Steps">Next Steps</a><a id="Next-Steps-1"></a><a class="docs-heading-anchor-permalink" href="#Next-Steps" title="Permalink"></a></h2><ul><li>Try <a href="../api/training.html#Episodic-Training">Episodic Training</a> for comparison</li><li>Learn about <a href="tutorials/@ref">Testing</a> your model</li><li>Explore <a href="tutorials/@ref">Hyperparameter Tuning</a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../quickstart.html">« Quick Start</a><a class="docs-footer-nextpage" href="episodic_training.html">Episodic Training »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 19 February 2026 08:31">Thursday 19 February 2026</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
